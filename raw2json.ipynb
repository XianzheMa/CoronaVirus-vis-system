{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is based on https://github.com/jianxu305/nCov2019_analysis. Credit the formatted data to him!\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHN_EN_DICT_ = './data/locationDict.csv'\n",
    "_DXY_DATA_PATH_ = 'https://raw.githubusercontent.com/BlankerL/DXY-2019-nCoV-Data/master/csv/DXYArea.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chinese_raw(path):\n",
    "    '''\n",
    "    This provides a way to look into the 'raw' data. If path is None, we fetch it from internet.\n",
    "    '''\n",
    "    if path == None:\n",
    "        path = _DXY_DATA_PATH_\n",
    "    raw = pd.read_csv(path)\n",
    "    \n",
    "    # the original CSV column names are in camel case, change to lower_case convention\n",
    "    rename_dict = {'updateTime': 'update_time',\n",
    "                   'provinceName': 'province_name',\n",
    "                   'cityName': 'city_name',\n",
    "                   'province_confirmedCount': 'province_confirmed',\n",
    "                   'province_suspectedCount': 'province_suspected',\n",
    "                   'province_deadCount': 'province_dead',\n",
    "                   'province_curedCount': 'province_cured',\n",
    "                   'city_confirmedCount': 'city_confirmed',\n",
    "                   'city_suspectedCount': 'city_suspected',\n",
    "                   'city_deadCount': 'city_dead',\n",
    "                   'city_curedCount': 'city_cured',\n",
    "                   #'provinceEnglishName': 'province_name_en',\n",
    "                   #'cityEnglishName': 'city_name_en'\n",
    "                  }\n",
    "    # drop zip columns\n",
    "    raw = raw.drop(columns = ['province_zipCode', 'city_zipCode'])\n",
    "    # temprorarily drop English name columns\n",
    "    raw = raw.drop(columns = ['provinceEnglishName', 'cityEnglishName'])\n",
    "    data = raw.rename(columns=rename_dict)\n",
    "    data['update_time'] = pd.to_datetime(data['update_time'])  # original type of update_time after read_csv is 'str'\n",
    "    data['update_date'] = data['update_time'].dt.date    # add date for daily aggregation, if without to_datetime, it would be a dateInt object, difficult to use\n",
    "    # display basic info\n",
    "    print('Last update: ', data['update_time'].max())\n",
    "    print('Data date range: ', data['update_date'].min(), 'to', data['update_date'].max())\n",
    "    print('Number of rows in raw data: ', data.shape[0])\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_cities(data):\n",
    "    '''\n",
    "    Sometimes, for example 2/3/2020, on some time snapshots, the CSV data contains city_name entries such as \"南阳\", \"商丘\", but at other time snapshots, it contains \"南阳（含邓州）\",  and \"商丘（含永城）\", etc.  They should be treated as the same city\n",
    "    This results in the aggregation on province level getting too high.\n",
    "    For now, entries will be ignored if city_name == xxx(xxx), and xxx already in the city_name set\n",
    "    '''\n",
    "    dup_frm = data[data['city_name'].str.contains('（')]\n",
    "    rename_dict = {name: name.split('（')[0] for name in dup_frm['city_name']}\n",
    "    dupCounty_frm = data[data['city_name'].str.endswith('县')]\n",
    "    dupCity_frm = data[data['city_name'].str.endswith('市')]\n",
    "    for name in dupCounty_frm['city_name']:\n",
    "        if len(name) >= 3: # for names with length 2 and ending with '县', like '滑县', no need to change\n",
    "            rename_dict[name] = name.split('县')[0]\n",
    "    for name in dupCity_frm['city_name']:\n",
    "        if len(name) >= 3:\n",
    "            rename_dict[name] = name.split('市')[0]\n",
    "    rename_dict['吐鲁番市'] = '吐鲁番'   # raw data has both 吐鲁番市 and 吐鲁番, should be combined\n",
    "    rename_dict['虹口'] = '虹口区'\n",
    "    rename_dict['嘉定'] = '嘉定区'\n",
    "    rename_dict['浦东'] = '浦东新区'\n",
    "    rename_dict['黄浦'] = '黄浦区'\n",
    "    rename_dict['杨浦'] = '杨浦区'\n",
    "    \n",
    "    rename_dict['浦东区'] = '浦东新区'\n",
    "    rename_dict['丰台'] = '丰台区'\n",
    "    rename_dict['静安'] = '静安区'\n",
    "    rename_dict['青浦'] = '青浦区'\n",
    "    rename_dict['大兴'] = '大兴区'\n",
    "    rename_dict['宝山'] = '宝山区'\n",
    "    rename_dict['徐汇'] = '徐汇区'\n",
    "    rename_dict['门头沟'] = '门头沟区'\n",
    "    rename_dict['闵行'] = '闵行区'\n",
    "    rename_dict['东城'] = '东城区'\n",
    "    rename_dict['通州'] = '通州区'\n",
    "    rename_dict['奉贤'] = '奉贤区'\n",
    "    rename_dict['红河州'] = '红河'\n",
    "    rename_dict['楚雄州'] = '楚雄'\n",
    "    rename_dict['大理州'] = '大理'\n",
    "    rename_dict['文山州'] = '文山'\n",
    "    rename_dict['德宏州'] = '德宏'\n",
    "    rename_dict['第八师石河子市'] = '兵团第八师石河子市'\n",
    "    rename_dict['第八师石河子'] = '兵团第八师石河子市'\n",
    "    rename_dict['石河子'] = '兵团第八师石河子市'\n",
    "    rename_dict['第八师'] = '兵团第八师石河子市'\n",
    "    rename_dict['第七师'] = '兵团第八师'\n",
    "    rename_dict['呼伦贝尔牙克石市'] = '呼伦贝尔牙克石'\n",
    "    rename_dict['通辽市经济开发区'] = '通辽'\n",
    "    rename_dict['第九师'] = '兵团第九师'\n",
    "    rename_dict['西双版纳州'] = '西双版纳'\n",
    "    rename_dict['滑县'] = ['滑县']\n",
    "    rename_dict['待明确'] = '待明确地区'\n",
    "    rename_dict['未明确地区'] = '待明确地区'\n",
    "    rename_dict['未知'] = '待明确地区'\n",
    "    rename_dict['未明确'] = '待明确地区'\n",
    "    rename_dict['未知地区'] = '待明确地区'\n",
    "    \n",
    "    data['city_name'] = data['city_name'].replace(rename_dict)  # write back\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_provinces(data):\n",
    "    '''\n",
    "    For simplicity, we only use simple names of provinces. For example, '河南省' will be renamed '河南'; '广西壮族自治区' will be renamed '广西'\n",
    "    '''\n",
    "    dup_frm = data[data['province_name'].str.contains('市|省')]\n",
    "    rename_dict = {name: name.split('市')[0] if '市' in name else name.split('省')[0] for name in dup_frm['province_name']}\n",
    "    rename_dict['内蒙古自治区'] = '内蒙古'\n",
    "    rename_dict['新疆维吾尔自治区'] = '新疆'\n",
    "    rename_dict['宁夏回族自治区'] = '宁夏'\n",
    "    rename_dict['广西壮族自治区'] = '广西'\n",
    "    rename_dict['西藏藏族自治区'] = '西藏'\n",
    "    rename_dict['西藏自治区'] = '西藏'\n",
    "    data['province_name'] = data['province_name'].replace(rename_dict)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_en_location(df):\n",
    "    '''Add province_name_en, and city_name_en, should only be called after calling rename_cities'''\n",
    "    chn_en = pd.read_csv(_CHN_EN_DICT_, encoding='utf-8')\n",
    "    translation = dict([t for t in zip(chn_en['Chinese'], chn_en['English'])])\n",
    "    df['province_name_en'] = df['province_name'].replace(translation)\n",
    "    df['city_name_en'] = df['city_name'].replace(translation)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggDaily(df):\n",
    "    '''Aggregate the frequent time series data into a daily frame, ie, one entry per (date, province, city)'''\n",
    "    frm_list = []\n",
    "    for key, frm in df.sort_values(['update_date']).groupby(['province_name', 'city_name', 'update_date']):\n",
    "        frm_list.append(frm.sort_values(['update_time'])[-1:])    # take the latest row within (city, date)\n",
    "    out = pd.concat(frm_list).sort_values(['update_date', 'province_name', 'city_name'])\n",
    "    out = out.drop(columns=\"update_time\")\n",
    "    out.sort_values(['province_name', 'city_name', 'update_date'], inplace=True)\n",
    "    return out.set_index(['province_name', 'city_name', 'update_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_virus_data(path = None):\n",
    "    '''\n",
    "    a pipline for load_chinese_raw, rename_cities add_en_location and aggDaily.\n",
    "    If path is None, we fetch is from internet.\n",
    "    Also, drop the entries with city_name '待明确地区'， '外地来*人员 for simplicity.\n",
    "    '''\n",
    "    data = load_chinese_raw(path)\n",
    "    data = rename_cities(data)\n",
    "    \n",
    "    dropped_indices = data[(data['city_name'] == '待明确地区') | (data['city_name'].str.contains('外地来'))].index\n",
    "    print(\"dropped {} rows with 待明确地区 or 外地来*人员 as city_name.\".format(len(dropped_indices)))\n",
    "    data.drop(dropped_indices, inplace = True)\n",
    "    \n",
    "    data = add_en_location(data)\n",
    "    data = rename_provinces(data)\n",
    "    return aggDaily(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing(df, report = True):\n",
    "    '''\n",
    "    At some date there are missing data. We complete it using the data at previous date.\n",
    "    If there are no available previous data, we set all numerical values 0.\n",
    "    'report' controls whether a completion report is printed out, defaults to True.\n",
    "    '''\n",
    "    indices = df.index\n",
    "    time_series = indices.get_level_values(2)\n",
    "    province_indices = set(indices.get_level_values(0)) # get the indices of province level\n",
    "    complete_time_indices = pd.date_range(time_series.min(), time_series.max(), freq='D') # the complete time indices\n",
    "    numerical_columns = [region +'_'+ case for region in ['province', 'city'] for case in ['confirmed', 'suspected', 'cured', 'dead']]\n",
    "    other_columns = set(df.columns) - set(numerical_columns) # other text based columns, such as English name\n",
    "    updated_provinces = []\n",
    "    num_completed = 0\n",
    "    for province in province_indices:\n",
    "        province_df = df.loc[province].copy() # get the sub dataframe for each province\n",
    "\n",
    "        city_indices = set(province_df.index.get_level_values(0)) # get the indices of city level within this province\n",
    "        updated_cities = []\n",
    "        for city in city_indices:\n",
    "            city_df = province_df.loc[city].copy() # get the sub dataframe for each city\n",
    "            time_indices = city_df.index\n",
    "            first = True # if the first date's data is missing, then this is used\n",
    "            for i, date in enumerate(complete_time_indices):\n",
    "                if date in time_indices:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                # the date at this date is missing\n",
    "                if first:\n",
    "                    first_date = time_indices[0]\n",
    "                    city_df.loc[date] = [0 if name in numerical_columns else city_df.loc[first_date, name] for name in city_df.columns]\n",
    "                else:\n",
    "                    city_df.loc[date] = city_df.loc[complete_time_indices[i - 1]].copy() # we copy it so that we can freely change one row without modifying others\n",
    "                if report:\n",
    "                    # messages printed got so long, change to a log file later\n",
    "                    # print(\"\\nmissing data at:\", province, city, str(date.year) +'-'+ str(date.month) +'-'+ str(date.day))\n",
    "                    # print(\"completed as:\\n\", city_df.loc[date].to_dict())\n",
    "                    num_completed += 1\n",
    "            city_df = city_df.sort_index(ascending = True)\n",
    "\n",
    "            updated_cities.append(pd.concat([city_df], keys=[city], names=['city_name'])) # prepend the city-level index and append it to updated_cities\n",
    "        updated_cities = pd.concat(updated_cities)\n",
    "        updated_cities = updated_cities.sort_index(ascending = True, level = 0) # sort city_name, just for consistency\n",
    "        updated_provinces.append(pd.concat([updated_cities], keys=[province], names=['province_name'])) # prepend the province-level index\n",
    "    new_df = pd.concat(updated_provinces).sort_index(ascending=True, level=0) # sort province_name, just for consistency\n",
    "    if report:\n",
    "        total_rows = new_df.shape[0]\n",
    "        print(\"completed {} rows out of {}\".format(num_completed, total_rows))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(data, file_name, start_date = None, end_date = None):\n",
    "    '''\n",
    "    output cleaned data to a json file within the time range [start_date, end-date].\n",
    "    start_date and end_date are two strings of format \"year-month-day\".\n",
    "    If they are not provided, use possibly largest time range in the data instead.\n",
    "    '''\n",
    "    # sorry for messy code\n",
    "    \n",
    "    multi_indices = data.index\n",
    "    time_series = multi_indices.get_level_values(2)\n",
    "    complete_min = time_series.min()\n",
    "    complete_max = time_series.max()\n",
    "    start_date = complete_min if start_date == None else max(pd.Timestamp(start_date), complete_min)\n",
    "    end_date = complete_max if end_date == None else min(pd.Timestamp(end_date), complete_max) # regularize start, end date\n",
    "    if start_date > end_date:\n",
    "        raise ValueError('start_date is larger than end_date')\n",
    "    print('actual time range used: ', start_date, end_date)\n",
    "    complete_time_indices = pd.date_range(start_date, end_date, freq='D') # the complete time indices\n",
    "    province_indices = set(multi_indices.get_level_values(0))\n",
    "    cases = ['confirmed', 'suspected', 'dead', 'cured']\n",
    "    province_cases = ['province_' + case for case in cases]\n",
    "    city_cases = ['city_' + case for case in cases]\n",
    "    \n",
    "    indent_level = 0\n",
    "    newline = lambda file, num_tabs: file.write('\\n' + ('\\t' * num_tabs)) # used to begin a new line with prescribed indentation\n",
    "    write_key_number = lambda file, key, number: file.write('\"' + key + '\": ' + str(number))\n",
    "    write_key_string = lambda file, key, string: file.write('\"' + key + '\": \"' + string + '\"')\n",
    "    write_key_date = lambda file, date: file.write('\"time\": \"' + str(date.month) +'-'+ str(date.day) +'-'+ str(date.year) + '\"')\n",
    "    write_date_colon = lambda file, date: file.write('\"' + str(date.month) +'-'+ str(date.day) +'-'+ str(date.year) + '\": ')\n",
    "    write_key_colon = lambda file, key: file.write('\"' + key + '\": ')\n",
    "\n",
    "    with open(file_name, \"w\") as f:        \n",
    "        f.write('{'); indent_level+=1; newline(f, indent_level)\n",
    "        for pro_id, province in enumerate(province_indices):\n",
    "            province_df = data.loc[province]\n",
    "            province_en = province_df['province_name_en'][0]\n",
    "            write_key_colon(f, province_en) # name of this province as the key\n",
    "            f.write('{'); indent_level+=1; newline(f, indent_level) # begin a province\n",
    "            \n",
    "            \n",
    "            write_key_string(f, \"Chinese_name\", str(province)) # the Chinese name of the province\n",
    "            f.write(','); newline(f, indent_level)\n",
    "\n",
    "            write_key_colon(f, \"cases\")\n",
    "            f.write('['); indent_level+=1; newline(f, indent_level) # begin province-level cases\n",
    "            \n",
    "            for date_id, date in enumerate(complete_time_indices):\n",
    "                f.write(\"{\"); indent_level+=1; newline(f, indent_level) # begin each time frame\n",
    "                write_key_date(f, date) # the time of each time frame\n",
    "                f.write(','); newline(f, indent_level)\n",
    "\n",
    "                for case_id, case in enumerate(province_cases):\n",
    "                    write_key_number(f, cases[case_id], province_df[case][date_id]) # the count for this case at this date\n",
    "                    if case_id < len(province_cases) - 1:\n",
    "                        f.write(','); newline(f, indent_level) # prepare for the next case\n",
    "                \n",
    "                indent_level-=1; newline(f, indent_level); f.write(\"}\") # end each time frame\n",
    "                if date_id < len(complete_time_indices) - 1:\n",
    "                    f.write(','); newline(f, indent_level) # prepare for next time frame\n",
    "                   \n",
    "            indent_level-=1; newline(f, indent_level); f.write(']') # end province-level cases\n",
    "            f.write(','); newline(f, indent_level)\n",
    "            \n",
    "            write_key_colon(f, \"cities\")\n",
    "            f.write('{'); indent_level+=1; newline(f, indent_level) # begin cities\n",
    "            city_indices = set(province_df.index.get_level_values(0))\n",
    "            for city_id, city in enumerate(city_indices):\n",
    "                city_en = province_df.loc[city]['city_name_en'][0]\n",
    "                write_key_colon(f, city_en) #  city name as the key\n",
    "                f.write(\"{\"); indent_level+=1; newline(f, indent_level) # begin each city\n",
    "                \n",
    "                write_key_string(f, 'Chinese_name', str(city)) # the Chinese name of this city\n",
    "                f.write(','); newline(f, indent_level);\n",
    "                write_key_colon(f, \"cases\")\n",
    "                f.write('['); indent_level+=1; newline(f, indent_level) # begin city-level cases\n",
    "                for date_id, date in enumerate(complete_time_indices):\n",
    "                    f.write(\"{\"); indent_level+=1; newline(f, indent_level) # begin each time frame\n",
    "                    write_key_date(f, date) # the time of each time frame\n",
    "                    f.write(','); newline(f, indent_level)\n",
    "                    for case_id, case in enumerate(city_cases):\n",
    "                        write_key_number(f, cases[case_id], province_df.loc[city][case][date_id]) # the count for this case at this date\n",
    "                        if case_id < len(city_cases) - 1:\n",
    "                            f.write(','); newline(f, indent_level) # prepare for the next case\n",
    "                \n",
    "                    indent_level-=1; newline(f, indent_level); f.write(\"}\") # end each time frame\n",
    "                    if date_id < len(complete_time_indices) - 1:\n",
    "                        f.write(','); newline(f, indent_level) # prepare for next time frame\n",
    "                indent_level-=1; newline(f, indent_level); f.write(\"]\") # end city-level cases\n",
    "                indent_level-=1; newline(f, indent_level); f.write(\"}\") # end each city\n",
    "                if city_id < len(city_indices) - 1:\n",
    "                    f.write(','); newline(f, indent_level) # prepare for the next city\n",
    "\n",
    "            indent_level-=1; newline(f, indent_level); f.write('}') # end cities\n",
    "            indent_level-=1; newline(f, indent_level); f.write(\"}\") # end a province\n",
    "            if pro_id < len(province_indices) - 1:\n",
    "                f.write(\",\"); newline(f, indent_level)  # prepare for next province\n",
    "                \n",
    "        indent_level-=1; newline(f, indent_level); f.write('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update:  2020-02-23 09:07:39.744000\n",
      "Data date range:  2020-01-24 to 2020-02-23\n",
      "Number of rows in raw data:  51836\n",
      "dropped 1006 rows with 待明确地区 or 外地来*人员 as city_name.\n"
     ]
    }
   ],
   "source": [
    "data = load_virus_data('./data/DXYArea_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2827 rows out of 14322\n"
     ]
    }
   ],
   "source": [
    "completed = complete_missing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual time range used:  2020-01-24 00:00:00 2020-02-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "to_json(completed, './data/DXYArea_long.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city_name  update_date\n",
       "万宁         2020-01-24     Henan\n",
       "           2020-01-25     Henan\n",
       "           2020-01-26     Henan\n",
       "           2020-01-27     Henan\n",
       "           2020-01-28     Henan\n",
       "                          ...  \n",
       "陵水         2020-02-09     Henan\n",
       "           2020-02-10     Henan\n",
       "           2020-02-11     Henan\n",
       "           2020-02-12     Henan\n",
       "           2020-02-13     Henan\n",
       "Name: province_name_en, Length: 378, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed.loc['海南']['province_name_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'临沧',\n",
       " '丽江',\n",
       " '丽江市',\n",
       " '保山',\n",
       " '大理',\n",
       " '大理州',\n",
       " '德宏',\n",
       " '德宏州',\n",
       " '文山',\n",
       " '文山州',\n",
       " '昆明',\n",
       " '昭通',\n",
       " '普洱',\n",
       " '曲靖',\n",
       " '楚雄',\n",
       " '楚雄州',\n",
       " '玉溪',\n",
       " '红河',\n",
       " '西双版纳'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(completed.loc['云南'].index.get_level_values(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
